{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))\n",
    "#Here is different categories that contains words those are most frequently or mostly used in that feild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Office', 'of', 'Business', 'Economics', '(', 'OBE', ')', 'of', 'the', 'U.S.', 'Department', 'of', 'Commerce', 'provides', 'basic', 'measures', 'of', 'the', 'national', 'economy', 'and', 'current', 'analysis', 'of', 'short-run', 'changes', 'in', 'the', 'economic', 'situation', 'and', 'business', 'outlook', '.'], ['It', 'develops', 'and', 'analyzes', 'the', 'national', 'income', ',', 'balance', 'of', 'international', 'payments', ',', 'and', 'many', 'other', 'business', 'indicators', '.'], ...]\n",
      "3032\n"
     ]
    }
   ],
   "source": [
    "data=brown.sents(categories='government')\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It develops and analyzes the national income , balance of international payments , and many other business indicators .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=' '.join(data[1])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to involve in this\n",
    "1.Get the data/corpus\n",
    "2.Tokenization, stopword removal\n",
    "3.Stemming/lemmatization\n",
    "4.Building a vocab\n",
    "5.Vectorization\n",
    "6.classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk tokenization & stopword removal\n",
    "document='''It was very pleasant day. The weather was cool and there were light showers.I went to market to buy some fruits. '''\n",
    "sentence='Send all the 50 documents related to chapters 1,2,3 at dipika@cb.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was very pleasant day.', 'The weather was cool and there were light showers.I went to market to buy some fruits.']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sentence_token=sent_tokenize(document)\n",
    "print(sentence_token)\n",
    "print(len(sentence_token))\n",
    "# as sent_tokenize separate the documents where it sees fullstop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'dipika', '@', 'cb.com']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(sentence)\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'any', 'be', 'when', 'hers', 'ain', 'my', 'being', \"couldn't\", 'been', 'their', 'own', 'some', 'hasn', 'through', 'me', \"haven't\", 'yourselves', 'with', 'doing', 'because', 'which', \"didn't\", \"wasn't\", 'down', 'mightn', 'she', 'its', 's', 'aren', 'other', 'these', 'there', 'and', \"shouldn't\", 'few', 'wasn', \"mightn't\", 'how', 'above', 'theirs', \"hasn't\", 'only', 'him', \"isn't\", 'those', 'into', \"aren't\", 'he', 'during', 'couldn', 'ma', 'no', \"hadn't\", 'to', 'yours', 'will', 'now', 'we', 'same', 'from', 'weren', \"doesn't\", 'is', 'wouldn', 'o', 'where', 'do', \"you'd\", 'a', 'after', 'll', 'what', \"you've\", 'by', 'whom', 'themselves', 'about', 'so', \"shan't\", 'too', 'am', 'why', 'or', 'not', 'ours', 'but', 'very', 'your', 'm', \"mustn't\", 'again', 'haven', 'while', 'most', 'himself', 'that', 'it', 'over', 'off', 'y', 'in', 'didn', 'yourself', 'each', 'for', 'once', 'his', 'at', 'on', \"she's\", 'if', 'd', 'they', \"that'll\", 'all', 'the', 'more', 'than', 'just', 'until', 'under', 'then', 'did', 'her', 'as', 'against', 'myself', \"wouldn't\", 'out', 'can', 'you', 'here', 'have', 'hadn', 'were', 'isn', 'this', 're', 'has', 'up', \"should've\", 'nor', 'are', 'needn', 'i', 'herself', 'was', 'before', 'below', 'doesn', 'an', 'of', 'who', 'our', 'mustn', 'such', 'shan', 'don', \"you'll\", 'shouldn', \"don't\", \"needn't\", \"it's\", 'further', 'both', 't', 'won', \"won't\", 'them', 'does', 've', \"weren't\", \"you're\", 'itself', 'having', 'should', 'ourselves', 'had', 'between'}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "sw=set(stopwords.words('english'))\n",
    "print(sw)\n",
    "print(len(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stopwords\n",
    "def remove_stopword(text,stopwords):\n",
    "    useful_words=[w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'pleasant',\n",
       " 'day.',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'cool',\n",
       " 'light',\n",
       " 'showers.I',\n",
       " 'went',\n",
       " 'market',\n",
       " 'buy',\n",
       " 'fruits.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopword(document.split(),sw)\n",
    "#use split orelse it will iterate with letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process of transforming verbs or plural words into their radical form\n",
    "Preserve the semantics of the sentence without increasing the number of unique tokens.\n",
    "Example=jumps,jumpping,jumped,jump==>jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer,RegexpStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "write\n",
      "hear\n",
      "noisi\n",
      "attract\n"
     ]
    }
   ],
   "source": [
    "text=['jumps','writing','hearing','noisy','attractive']\n",
    "ps=PorterStemmer()\n",
    "for word in text:\n",
    "    word=ps.stem(word.lower())\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attract\n",
      "beautiful\n",
      "beauti\n"
     ]
    }
   ],
   "source": [
    "sb=SnowballStemmer('english')\n",
    "print(sb.stem('attractive'))\n",
    "print(SnowballStemmer('french').stem('beautiful'))\n",
    "print(SnowballStemmer('english').stem('beautiful'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "the objective of reducing a word to its base form and grouping together different forms of the same word.\n",
    "For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and \n",
    "synonyms are unified (e.g. “best” is changed to “good”), \n",
    "hence standardizing words with similar meaning to their root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Vocab & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocab & vectorization\n",
    "corpus=[\n",
    "    '''When compared to other countries with similar or higher case load, India called \n",
    "    its strict lockdown at a much earlier point on its case and death curves. \n",
    "    These 18 other countries had more than 500 cases when they called their strictest lockdown, \n",
    "    while India had 320. Again, India had only four deaths on March 22, \n",
    "    when its score reached 100, while most countries had more deaths at that point (except Switzerland; no deaths). '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '18', '22', '320', '500', 'again', 'and', 'at', 'called', 'case', 'cases', 'compared', 'countries', 'curves', 'death', 'deaths', 'earlier', 'except', 'four', 'had', 'higher', 'india', 'its', 'load', 'lockdown', 'march', 'more', 'most', 'much', 'no', 'on', 'only', 'or', 'other', 'point', 'reached', 'score', 'similar', 'strict', 'strictest', 'switzerland', 'than', 'that', 'their', 'these', 'they', 'to', 'when', 'while', 'with']\n"
     ]
    }
   ],
   "source": [
    "vectorize=CountVectorizer()\n",
    "vectorized_corpus=vectorize.fit_transform(corpus)\n",
    "print(vectorize.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x50 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorized_corpus.shape)\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, 3,\n",
       "        3, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 3, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus=vectorized_corpus.toarray()\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'when': 47, 'compared': 11, 'to': 46, 'other': 33, 'countries': 12, 'with': 49, 'similar': 37, 'or': 32, 'higher': 20, 'case': 9, 'load': 23, 'india': 21, 'called': 8, 'its': 22, 'strict': 38, 'lockdown': 24, 'at': 7, 'much': 28, 'earlier': 16, 'point': 34, 'on': 30, 'and': 6, 'death': 14, 'curves': 13, 'these': 44, '18': 1, 'had': 19, 'more': 26, 'than': 41, '500': 4, 'cases': 10, 'they': 45, 'their': 43, 'strictest': 39, 'while': 48, '320': 3, 'again': 5, 'only': 31, 'four': 18, 'deaths': 15, 'march': 25, '22': 2, 'score': 36, 'reached': 35, '100': 0, 'most': 27, 'that': 42, 'except': 17, 'switzerland': 40, 'no': 29}\n"
     ]
    }
   ],
   "source": [
    "print(vectorize.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorize.vocabulary_))\n",
    "print(len(vectorize.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Reverse Mapping\n",
    "print(len(vectorized_corpus))\n",
    "print(type(vectorized_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 3, 1, 1, 3, 1, 1, 1, 4, 1, 3,\n",
       "        3, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 3, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['100', '18', '22', '320', '500', 'again', 'and', 'at', 'called',\n",
       "        'case', 'cases', 'compared', 'countries', 'curves', 'death',\n",
       "        'deaths', 'earlier', 'except', 'four', 'had', 'higher', 'india',\n",
       "        'its', 'load', 'lockdown', 'march', 'more', 'most', 'much', 'no',\n",
       "        'on', 'only', 'or', 'other', 'point', 'reached', 'score',\n",
       "        'similar', 'strict', 'strictest', 'switzerland', 'than', 'that',\n",
       "        'their', 'these', 'they', 'to', 'when', 'while', 'with'],\n",
       "       dtype='<U11')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=vectorize.inverse_transform(vectorized_corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(corpus))\n",
    "s=vectorized_corpus.tobytes()\n",
    "s=str(s)\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tokenizer that splits a string using a regular expression, which\n",
    "# matches either the tokens or the separators between tokens.\n",
    "\n",
    "#Indian and indian are same\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mytoken(document,stopwords):\n",
    "    word=tokenizer.tokenize(document.lower())\n",
    "    #remove stopwords\n",
    "    word=[w for w in document if w not in stopwords]\n",
    "    #word=\" \".join(word)\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= '''When compared to other countries with similar or higher case load, India called \n",
    "    its strict lockdown at a much earlier point on its case and death curves. \n",
    "    These 18 other countries had more than 500 cases when they called their strictest lockdown, \n",
    "    while India had 320. Again, India had only four deaths on March 22, \n",
    "    when its score reached 100, while most countries had more deaths at that point (except Switzerland; no deaths). '''\n",
    "\n",
    "token=Mytoken(corpus,set(stopwords.words('english')))\n",
    "#token=','.join(token)\n",
    "#token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this is not good movie']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1=['this is good movie']\n",
    "sent_2=['this is not good movie']\n",
    "doc=[sent_1[0],sent_2[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1]\n",
      " [1 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'not': 3}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cv.fit_transform(doc).toarray())\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(ngram_range=(3,3))\n",
    "cv.fit_transform(doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is good': 3,\n",
       " 'is good movie': 0,\n",
       " 'this is not': 4,\n",
       " 'is not good': 1,\n",
       " 'not good movie': 2}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this is not good movie', 'this was good movie']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_3=['this was good movie']\n",
    "corpus=[sent_1[0],sent_2[0],sent_3[0]]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]]\n"
     ]
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "vc=tfidf.fit_transform(corpus).toarray()\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'not': 3, 'was': 5}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]]\n"
     ]
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer(ngram_range=(2,2))\n",
    "vc=tfidf.fit_transform(corpus).toarray()\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 4,\n",
       " 'is good': 1,\n",
       " 'good movie': 0,\n",
       " 'is not': 2,\n",
       " 'not good': 3,\n",
       " 'this was': 5,\n",
       " 'was good': 6}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
